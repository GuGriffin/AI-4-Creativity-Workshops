{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4C: Train Keypoints Classifier, i.e. Pose Classifier\n",
    "\n",
    "In this notebook we are going to train our own pose classifier in PyTorch based on the dataset we built from the python script and notebook `00` and `01` respectively.\n",
    "\n",
    "**Before you go any further** make sure you have already created and saved your csv file in `class-datasets/my-pose-classification-dataset`.\n",
    "\n",
    "Code adapted from this [repo](https://github.com/Alimustoofaa/YoloV8-Pose-Keypoint-Classification/tree/master)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up your Python environment\n",
    "\n",
    "Before you work through this notebook, please follow the instructions in [Setup-and-test-conda-environment.ipynb](Setup-and-test-conda-environment.ipynb)\n",
    "\n",
    "Once you have done that you will need to make sure that the environment selected to run this notebook and all the other notebooks used in this unit is called `aim`. \n",
    "\n",
    "To do this click the **Select kernel** button in the top right corner of this notebook, and then select `aim`.\n",
    "\n",
    "To make sure that is configured properly, Hit the run cell button (â–¶) on the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.environ['CONDA_DEFAULT_ENV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it output the text `aim`?\n",
    "\n",
    "If it does not output the text `aim`, please revisit and follow the instructions in [Setup-and-test-conda-environment.ipynb](Setup-and-test-conda-environment.ipynb).\n",
    "\n",
    "If you still cannot get it working, please raise this with the course instructor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can import the libraries you need to train the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameters\n",
    "\n",
    "Now let's define our hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "num_epochs = 200\n",
    "num_classes = 3\n",
    "num_keypoints = 34\n",
    "test_size = 0.3\n",
    "batch_size = 128\n",
    "learn_rate = 0.001\n",
    "data_path = 'class-datasets/my-pose-classification-dataset/poses_keypoints.csv'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read Dataset\n",
    "\n",
    "Here, we are reading the first 5 rows of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path)\n",
    "df = df.drop('image_name', axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Count and plot our data per class\n",
    "\n",
    "In the following two cells, we are counting and plotting the number of data per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.label.value_counts().plot(kind=\"bar\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define the 1st column as our labels `y` and the following 34 columns as our keypoints input dataset `X`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the encoder label, to turn each label into an index number\n",
    "encoder = LabelEncoder()\n",
    "y_label = df['label']\n",
    "y = encoder.fit_transform(y_label)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get keypoint dataset \n",
    "X = df.iloc[:, 1:] # start from 11: if you want to skip the keypoints of the face\n",
    "X"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train Test Split\n",
    "\n",
    "Perform a train-test split with test_size=0.3 (defined in our hyperparameters), and a random but deterministic split and a strification.\n",
    "\n",
    "Stratified sampling is a method of sampling that involves dividing a population into homogeneous subgroups known as strata, and then sampling from each stratum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Number of Training keypoints: \", len(X_train))\n",
    "print(\"Number of Testing keypoints: \", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A glimpse into the test data in a table format\n",
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MinMax scaling to scale each feature into a given range\n",
    "\n",
    "For more information, look [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A glipse into the test data in the format of an array and after performing a minmax scaling\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Loader\n",
    "\n",
    "The data are currently numpy arrays and need to get transformed into torch tensors in order to get into the dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataKeypointClassification(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.x = torch.from_numpy(X.astype(np.float32))\n",
    "        self.y = torch.from_numpy(y.astype(np.int64))\n",
    "        self.n_samples = X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DataKeypointClassification(X_train, y_train)\n",
    "test_dataset = DataKeypointClassification(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,  shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define our simple feed forward network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseClassificationMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "      super(PoseClassificationMLP, self).__init__()\n",
    "      self.fc1 = nn.Linear(num_keypoints, 256)\n",
    "      self.fc2 = nn.Linear(256, num_classes)     \n",
    "  \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.softmax(x, dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup core objects\n",
    "\n",
    "Here we setup our core objects, the model, the loss function and the optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PoseClassificationMLP()\n",
    "model.to(device)\n",
    "\n",
    "# Cross entropy loss for training classification\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Adam optimiser\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training loop\n",
    "\n",
    "Here is our training loop for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "best_loss = 100000\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    # Training loop\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # Get data\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Process data\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Update model weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    # Added cumulative losses to lists for later display\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}, train loss: {train_loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Train loss\")\n",
    "plt.plot(train_losses,label=\"train\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"cumulative loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test our model\n",
    "\n",
    "Here we use the model to predict the label on unseen data, our test data.\n",
    "\n",
    "The predictions are the predicted classes (in the encoded format 0-1-2) for each item in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = torch.from_numpy(X_test.astype(np.float32))\n",
    "test_labels = y_test\n",
    "with torch.no_grad():\n",
    "    outputs = model(test_features)\n",
    "    _, predictions = torch.max(outputs, 1)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confusion Matrix\n",
    "\n",
    "A confusion matrix is a really good way to visualise the number of true positives, false negatives, false positives, and true negatives.\n",
    "\n",
    "The header row corresponds to the predicted labels while the first column corresponds to the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(test_labels, predictions)\n",
    "df_cm = pd.DataFrame(\n",
    "    cm, \n",
    "    index = encoder.classes_,\n",
    "    columns = encoder.classes_\n",
    ")\n",
    "df_cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualising the confusion matrix with a seaborn heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion_matrix(confusion_matrix):\n",
    "    hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.ylabel(\"Surface Ground Truth\")\n",
    "    plt.xlabel(\"Predicted Surface\")\n",
    "    plt.legend()\n",
    "    \n",
    "show_confusion_matrix(df_cm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_SAVE = 'pose_classifier.pt'\n",
    "torch.save(model.state_dict(), PATH_SAVE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Inference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inference =  PoseClassificationMLP()\n",
    "model_inference.load_state_dict(torch.load(PATH_SAVE, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature, label = test_dataset.__getitem__(12) # test out different item numbers\n",
    "\n",
    "out = model_inference(feature)\n",
    "_, predict = torch.max(out, -1)\n",
    "print(f'\\\n",
    "    prediction label : {encoder.classes_[predict]} \\n\\\n",
    "    ground truth label : {encoder.classes_[label]}'\n",
    "    )\n",
    "print(encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training tasks:\n",
    "\n",
    "**Task A:** Run all the cells in this code to train your own pose classifier.\n",
    "\n",
    "> There are some bonus tasks here if you want to further develop skills in training models. Feel free to come back to these after completing the tasks for building the interactive application.\n",
    "> \n",
    "> **Bonus task A:** Use Sci-kit learn's [train-test split function](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to split this dataset into a train and test/validation dataset. Every 10 epochs assess the perfomance of the model against the test set (like in [week-2b](Week-2b-build-and-train-dog-rating-network.ipynb)) and use early stopping to save and evaluate the most performant model.\n",
    "> \n",
    "> **Bonus task B:** Test out the training with different hyper-parameters, or make modifications to the network architecture to try and improve classification accuracy on the test set.\n",
    "\n",
    "### Interactive app tasks \n",
    "\n",
    "In [week-4d-pose-classification-dorothy-app.py](week-4d-pose-classification-dorothy-app.py) follow the tasks below to \n",
    "\n",
    "**Task 1:** Copy and paste [the model definition](#define-our-simple-feed-forward-network) from this notebook into [week-4d-pose-classification-dorothy-app.py](week-4d-pose-classification-dorothy-app.py). Then instantiate a copy of that class, load the weights of the model saved into `pose_classifier.pt` using [the torch load functionality](https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference) and then put the model into eval mode.\n",
    "\n",
    "**Task 2** Run your pose classifier model in inference in the draw loop (in the designated part of the code) on the variable `keypoint_data`. The use the `torch.max` function to get prediction for the pose of the user. \n",
    "\n",
    "**Task 3:** Use the predicted pose to alter the behaviour or trigger an action in the dorothy sketch. Each pose should have a different effect. For instance you could: \n",
    "- Draw text or geometric objects\n",
    "- Manipulate animation of graphics on the screen\n",
    "- Trigger the playback of audio\n",
    "- Create a game where the user has to match a pose on command, there could be a time limit or points could be scored for how quickly the user responds to the instructed pose from the game.\n",
    "\n",
    "If you did STEM for Creatives last term, can you adapt one of the sketches you created last term and use this pose classifer as a controller to interact with that sketch in some way?\n",
    "\n",
    "For inspiration, checkout the [gallery of examples given in the dorothy-cci library](https://github.com/Louismac/dorothy/tree/main/examples)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
