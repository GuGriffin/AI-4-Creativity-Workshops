{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3b: Downloading and embedding a museum dataset\n",
    "\n",
    "This notebook follows on from [Week-3a-Embeddings-and-cosine-similarity.ipynb](Week-3a-Embeddings-and-cosine-similarity.ipynb), which you should read through before starting the tasks in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up your Python environment\n",
    "\n",
    "Before you work through this notebook, please follow the instructions in [Setup-and-test-conda-environment.ipynb](Setup-and-test-conda-environment.ipynb)\n",
    "\n",
    "Once you have done that you will need to make sure that the environment selected to run this notebook and all the other notebooks used in this unit is called `aim`. \n",
    "\n",
    "To do this click the **Select kernel** button in the top right corner of this notebook, and then select `aim`.\n",
    "\n",
    "To make sure that is configured properly, Hit the run cell button (â–¶) on the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.environ['CONDA_DEFAULT_ENV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it output the text `aim`?\n",
    "\n",
    "If it does not output the text `aim`, please revisit and follow the instructions in [Setup-and-test-conda-environment.ipynb](Setup-and-test-conda-environment.ipynb).\n",
    "\n",
    "If you still cannot get it working, please raise this with the course instructor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscrape data from the Met's website\n",
    "\n",
    "The following cell runs the python script [download-met-museum-data.py](util/download-met-museum-data.py) located in the [util](util) directory. This code will download scrape images from [The Metropolitan Museum of Art website](https://www.metmuseum.org/). This will randomly download images from page items on this website [like this one](https://www.metmuseum.org/art/collection/search/15000). \n",
    "\n",
    "Each time you run the following script you will download a different set of images. By default it will try to find and download 100 data samples at a time, though you can increase this by changing the number that follows the flag `--num_samples`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python util/download-met-museum-data.py --num_samples 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the folder [class-datasets/met_images](class-datasets/met_images) in the file navigation program for your operating system to see the images that have been downloaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process museum image dataset with CLIP\n",
    "\n",
    "The first part of the task today is to calculate an embedding for each image in the dataset you have just scraped from the Met's collection.\n",
    "\n",
    "First you need to do some imports and load in the same clip model used in the previous notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import open_clip\n",
    "\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='openai')\n",
    "model.eval()\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Write a function to get image embeddings\n",
    "\n",
    "Your first task is to write a function that returns an image embedding based on a PIL image. You will need to refer back to the sequence of steps taken in Part 3 of [Week-3a-Embeddings-and-cosine-similarity.ipynb](Week-3a-Embeddings-and-cosine-similarity.ipynb) in order to do this. \n",
    "\n",
    "The following function `get_clip_embedding_from_PIL_image` should take a PIL image as input, convert it to a torch tensor, calcuate an embedding with CLIP, then conver this back to a numpy array, which should contain a vector of length $512$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clip_embedding_from_PIL_image(image):\n",
    "    #\n",
    "    # Your code goes here\n",
    "    #\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will run a test your function to see if you have done this correctly. \n",
    "\n",
    "If you get an error, you will need to inspect the error to see what the problem is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = Image.open('media/golden-retriever.jpg').convert(\"RGB\")\n",
    "\n",
    "embedding = get_clip_embedding_from_PIL_image(test_image)\n",
    "\n",
    "assert type(embedding) == np.ndarray\n",
    "assert embedding.shape == (512,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Process all images with CLIP \n",
    "\n",
    "You now need to write a function that will process every image in the dataset with the function you wrote. You will need to add these embeddings to a python list called `embedding_list`. This function will also keep track of the image id of every image (extracted from the filename), which will be put into a list called `id_list` (this is done for you):\n",
    "\n",
    "The function will take a path the an image directory as input and return the two variables `embedding_list` and `id_list`, which should be python lists (containing numpy arrays and ints respectively).\n",
    "\n",
    "Complete the code for the function below. Inside the `try` block write code that performs the following steps:\n",
    "1. loads a PIL image from the `image_path` variable\n",
    "2. Gets a vector embedding using the function from task 1\n",
    "3. Appends this `embedding` to the list `embedding_list`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(image_directory):\n",
    "    embedding_list = []\n",
    "    id_list = []\n",
    "\n",
    "    for image_name in os.listdir(image_directory):\n",
    "        image_path = os.path.join(image_directory, image_name)\n",
    "        item_id = os.path.splitext(image_name)[0]\n",
    "        id_list.append(int(item_id))\n",
    "        try:\n",
    "            #\n",
    "            # Code for \n",
    "            #\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_name}: {e}\")\n",
    "        \n",
    "    return embedding_list, id_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will test your function. If you get an error then you will need to look at the error to determine what has gone wrong:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_list, id_list = generate_embeddings('class-datasets/met_images')\n",
    "\n",
    "assert len(embedding_list) > 1\n",
    "assert len(embedding_list) == len(id_list)\n",
    "assert type(embedding_list[0]) == np.ndarray\n",
    "assert embedding_list[0].shape == (512,)\n",
    "assert type(id_list[0]) == int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Convert and save data arrays\n",
    "\n",
    "The next task is to convert these lists into numpy arrays. Pass these list variables as parameters into the function `np.array( )` to create the new variables `embedding_matrix_np` & `id_array_np`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_np = \n",
    "id_array_np = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_np = np.array(embedding_list)\n",
    "id_array_np = np.array(id_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will test your function. If you get an error then you will need to look at the error to determine what has gone wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(embedding_matrix_np) == np.ndarray\n",
    "assert type(id_array_np) == np.ndarray\n",
    "assert embedding_matrix_np.shape[0] == id_array_np.shape[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have now a matrix that contains all the vector embeddings for images in your dataset. Each row in this matrix is it's own embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding_matrix_np.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Search image embeddings with text\n",
    "\n",
    "You now have created a matrix of image embeddings. You can now use the cosine distance as before to compare the compare the similarity of a new vector embedding for a text string, against the embeddings for the images you have calculated. \n",
    "\n",
    "Lets get a CLIP embedding for a generic text description of an item:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_description = 'an antique vase'\n",
    "\n",
    "text_tensor = open_clip.tokenizer.tokenize(item_description)\n",
    "with torch.no_grad():\n",
    "    text_embedding = model.encode_text(text_tensor).squeeze().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your job is to compare this text embedding vector with all the image embedding vectors in the matrix `embedding_matrix_np`. You could do this in a for loop, however there is a much faster way. As this operation is essentially just taking the dot product of a vector with every row of a matrix (which you did in [Week 1](Week-1-Neural-networks-in-numpy-solutions.ipynb)).\n",
    "\n",
    "This following function is almost identical the function `get_cosine_similarity` in the previous notebook, if you are interested you can compare them to see what changes have been made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_cosine_similarities(embeddings_matrix, embedding_vector):\n",
    "        dot_product = embeddings_matrix @ embedding_vector\n",
    "        product_of_magnitudes = np.linalg.norm(embeddings_matrix, axis = 1) * np.linalg.norm(embedding_vector)\n",
    "        return dot_product / product_of_magnitudes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can test this function on your matrix of image embeddings. This will give a list all the cosine similarities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim_array = get_all_cosine_similarities(embedding_matrix_np, text_embedding)\n",
    "print(cosine_sim_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the function `np.argmax( )` to create the variable `highest_score_index` that contains the index of the maximum value in the array of cosine similarities. This index will be used to retrieve the item id from the variable `id_list`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id_for_most_similar_item(similarity_array, id_list):\n",
    "    highest_score_index = \n",
    "    item_id = id_list[highest_score_index]\n",
    "    return item_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will test your function. If you get an error then you will need to look at the error to determine what has gone wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_match_id = get_id_for_most_similar_item(cosine_sim_array, id_list)\n",
    "assert type(closest_match_id) == int\n",
    "assert closest_match_id in id_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can take a look at the image and see for yourself how similar it is to the text description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = f'class-datasets/met_images/{closest_match_id}.jpg'\n",
    "closest_match = Image.open(image_path).convert(\"RGB\")\n",
    "print(f\"closest match for text string: '{item_description}' is the image: {image_path}\")\n",
    "closest_match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Save numpy arrays to file and install library\n",
    "\n",
    "You will now take what you have done and add the functions and import the embedding arrays into the [sketchy collections streamlit app](week-3c-sketchy-collections-streamlit-app.py). \n",
    "\n",
    "First you need to save the numpy arrays to external files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('demo/embeddings/', exist_ok=True)\n",
    "np.save('demo/embeddings/met_embeddings.npy', embedding_matrix_np)\n",
    "np.save('demo/embeddings/met_embedding_ids.npy', id_array_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the app `week-3c-sketchy-collections-streamlit-app.py` you need to install the following library, for taking sketches as input to the app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install streamlit-drawable-canvas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6: Copy functions from this notebook to the app\n",
    "\n",
    "You now need to take the code you wrote for the tasks here and import them into [week-3c-sketchy-collections-streamlit-app.py](week-3c-sketchy-collections-streamlit-app.py).\n",
    "\n",
    "Copy and past your implementations for the functions that you wrote:\n",
    "\n",
    "- `get_clip_embedding_from_PIL_image`\n",
    "- `get_id_for_most_similar_item`\n",
    "\n",
    "into the designated space in [week-3c-sketchy-collections-streamlit-app.py](week-3c-sketchy-collections-streamlit-app.py).\n",
    "\n",
    "### Task 7: Run your streamlit app\n",
    "\n",
    "You can then run this app to interactively explore the archive you have downloaded. As this app is using the library [streamlit](https://streamlit.io/). Running streamlit apps is differnt to how you normally run python scripts. You need to run this in the correct CLI programme for your operating system, for more information on how to do that follow the steps in [Instructions-for-running-streamlit-apps.ipynb](Instructions-for-running-streamlit-apps.ipynb).\n",
    "\n",
    "Once you in the correct python CLI programme for your PC, in the `aim` conda environment, and in this directory, you can run the command\n",
    "```\n",
    "streamlit run week-3c-sketchy-collections-streamlit-app.py\n",
    "```\n",
    "to search your webscraped museum archive dataset using drawing as an input!\n",
    "\n",
    "This app uses user the CLIP image embeddings of sketches (instead of text) to search the museum dataset, but the code and principal here is identical to what you did in [task 4](#task-4-search-image-embeddings-with-text).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
