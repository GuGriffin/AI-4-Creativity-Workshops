{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2b - build and train a dog rating network\n",
    "\n",
    "In this code we will see how to train a dog rating network using the [we rate dogs dataset](https://www.kaggle.com/datasets/terencebroad/we-rate-dogs-images-ratings-and-captions).\n",
    "\n",
    "First lets check you have the write environment setup:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up your Python environment\n",
    "\n",
    "Before you work through this notebook, please follow the instructions in [Setup-and-test-conda-environment.ipynb](Setup-and-test-conda-environment.ipynb)\n",
    "\n",
    "Once you have done that you will need to make sure that the environment selected to run this notebook and all the other notebooks used in this unit is called `aim`. \n",
    "\n",
    "To do this click the **Select kernel** button in the top right corner of this notebook, and then select `aim`.\n",
    "\n",
    "To make sure that is configured properly, Hit the run cell button (â–¶) on the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aim\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ['CONDA_DEFAULT_ENV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it output the text `aim`?\n",
    "\n",
    "If it does not output the text `aim`, please revisit and follow the instructions in [Setup-and-test-conda-environment.ipynb](Setup-and-test-conda-environment.ipynb).\n",
    "\n",
    "If you still cannot get it working, please raise this with the course instructor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "First we need to import torch and the various other utilities in torch for building and training models, and loading data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to import the WeRateDogsDataset class from [util/we_rate_dogs_dataset.py](util/we_rate_dogs_dataset.py) that has been built to load and process this custom dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.we_rate_dogs_dataset import WeRateDogsDataset\n",
    "# Import data loader class from: https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup parameters for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "momentum = 0.9\n",
    "batch_size = 100\n",
    "learn_rate = 0.001\n",
    "data_path = 'class-datasets/we-rate-dogs-mini/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations for pre-procesing the image data into the right format for the model to process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A breakdown of what is going on here is given in the slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [   \n",
    "        torchvision.transforms.Resize(10, antialias=True),\n",
    "        torchvision.transforms.CenterCrop(10),\n",
    "        torchvision.transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5)),\n",
    "        torch.flatten\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define dataset and dataloader objects\n",
    "\n",
    "In torch a dataset class defines what and where our dataset is, and a dataloader class defines how we load that data into batches when training.\n",
    "\n",
    "Here we have two of each, one for our training dataset that the model is trained on, and one for our test dataset that we will use to evaluate the accuracy of our models at regular intervals: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = WeRateDogsDataset(data_path, 'train', transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = WeRateDogsDataset(data_path, 'test', transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the network architecture\n",
    "\n",
    "Here you will need to define the architecture for the network. Use the notebook [Week-2a-basic-MLP-PyTroch.ipynb](Week-2a-basic-MLP-PyTroch.ipynb) as a reference. This network will need to take a vector of dimensionality 100 into the first layer, and have a single scalar output in the final layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DogRatingNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DogRatingNetwork, self).__init__()\n",
    "        # Define network architecture here\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define the forward pass of the network here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define core objects for training\n",
    "\n",
    "Here we instantiate the three core objects for any training in PyTorch:\n",
    "- The neural network model\n",
    "- The loss function (aka criterion)\n",
    "- The optimiser (for updating the weights of the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DogRatingNetwork()\n",
    "model.to(device)\n",
    "criterion = nn.L1Loss() \n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "\n",
    "Here is a basic training loop in PyTorch. `num_epochs` defines the number of full iterations we take through the dataset. With the for loop where we go through each epoch there are two sub-loops:\n",
    "- **Training loop:** here we cycle through the training dataset, calculate the loss and use that to update the weights of the network\n",
    "- **Testing loop:** every 10 epochs we cycle through the test dataset and calculate the total loss, here we do not update the weights of the network, as we need to use *unseen data* to properly evaluate our networks performance and avoid *overfitting*.\n",
    "\n",
    "This code will automatically find the model checkpoint that has the best score on the test dataset and save that to the file `best_dog_rating_network.pt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = 100000\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs): \n",
    "    train_loss = 0.0\n",
    "    test_loss = 0.0\n",
    "    \n",
    "    # Training loop\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() / len(train_loader)\n",
    "\n",
    "    # After 10 epochs\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        # Test loop\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(test_loader, 0):\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item() / len(test_loader)\n",
    "\n",
    "        print(f'Epoch {epoch + 1}, train loss: {train_loss:.3f}, test loss: {test_loss:.3f}')\n",
    "        if test_loss < best_loss:\n",
    "            best_loss = test_loss\n",
    "            torch.save(model.state_dict(), 'best_dog_rating_network.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load best model\n",
    "\n",
    "Now lets reload the best preforming model that we saved to our checkpoint file, and overwrite the current weights of our model with the best set of weights we have found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('best_dog_rating_network.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a new image from the web\n",
    "\n",
    "Here we are loading in a new image of a dog to test our trained model, try finding a different image from the web and loading it into here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "dog_url = 'https://hips.hearstapps.com/hmg-prod/images/dog-puppy-on-garden-royalty-free-image-1586966191.jpg?crop=0.752xw:1.00xh;0.175xw,0&resize=1200:*'\n",
    "im = Image.open(requests.get(dog_url, stream=True).raw)\n",
    "im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model on new image\n",
    "\n",
    "Now lets test our model on this new data and see how it rates the dog:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_im = transform(im)\n",
    "torch_im = torch_im.unsqueeze(0)\n",
    "pred = model(torch_im)\n",
    "print(f'We give this doggo a rating of {pred.item():.2f}/10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks \n",
    "\n",
    "**Task 1:** [Build a simple MLP model in this cell](#define-the-network-architecture) with one hidden layer followed by a output layer. The input layer should be 100 units, the first hidden layer should have 10 units, and the output layer should have 1 unit. Then hit **run all** at the top of your notebook to run training. Use the notebook [Week-2a-basic-MLP-PyTroch.ipynb](Week-2a-basic-MLP-PyTroch.ipynb) as a reference.\n",
    "\n",
    "**Task 2:** Adapt this MLP model to make use of biases and activation functions:\n",
    "-  In `__init__` adapt the code to add biases to the units in the hidden layer\n",
    "-  In `forward` use the [pytorch RELU (rectified linear unit)](https://pytorch.org/docs/main/generated/torch.nn.ReLU.html#relu) on the outputs outputs of the hidden layer ([see this blog for reference code](https://eitca.org/artificial-intelligence/eitc-ai-dlpp-deep-learning-with-python-and-pytorch/data-eitc-ai-dlpp-deep-learning-with-python-and-pytorch/datasets/what-is-the-relu-function-in-pytorch/)).\n",
    "\n",
    "**Task 3a:** Now try adding more layers to your network, and changing the number of units in each hidden layer? Can you reduce the test error by increasing the size of the network models? Keep track of your lowest score on the test set and upload your best score to the leaderboard (link on this weeks lesson plan on moodle).\n",
    "\n",
    "**Task 3b:** Now try experimenting with [different activation functions](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity) or adapting [the parameters for training](#setup-parameters-for-training-the-model), by adjusting the `momentum`, `learning_rate`, or `num_epochs` used for training. Keep track of your lowest score on the test set and upload your best score to the leaderboard (link on this weeks lesson plan on moodle).\n",
    "\n",
    "**Task 4:** Find some new cute pictures of dogs on google image search (or search engine of your choice). What is the highest score you can find for a new dog picture you have found? Upload the dog and rating to the leaderboard (link on this weeks lesson plan on moodle).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
