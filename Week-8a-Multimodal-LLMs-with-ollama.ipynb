{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8a: Multimodal LLMs with Ollama\n",
    "\n",
    "Today you will be using Ollama to download and run state of the art multi-modal LLMs. \n",
    "\n",
    "Ollama has an online database of recent state of the art open-weight and open-source LLMs models, such as Meta's LLAMA class of models, Deep Seek, Mistral. You can see the list of models available here: https://ollama.com/search\n",
    "\n",
    "With Ollama LLMs models can be interacted with in their standalone CLI software, or you can connect Ollama to Python code using [their python api](https://pypi.org/project/ollama/) to build interactive software interfaces and other LLM powered applications in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up your Python environment\n",
    "\n",
    "Before you work through this notebook, please follow the instructions in [Setup-and-test-conda-environment.ipynb](Setup-and-test-conda-environment.ipynb)\n",
    "\n",
    "Once you have done that you will need to make sure that the environment selected to run this notebook and all the other notebooks used in this unit is called `aim`. \n",
    "\n",
    "To do this click the **Select kernel** button in the top right corner of this notebook, and then select `aim`.\n",
    "\n",
    "To make sure that is configured properly, Hit the run cell button (â–¶) on the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.environ['CONDA_DEFAULT_ENV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it output the text `aim`?\n",
    "\n",
    "If it does not output the text `aim`, please revisit and follow the instructions in [Setup-and-test-conda-environment.ipynb](Setup-and-test-conda-environment.ipynb).\n",
    "\n",
    "If you still cannot get it working, please raise this with the course instructor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Ollama\n",
    "\n",
    "Download the standalone Ollama application for your operating system from the following url: https://ollama.com/download\n",
    "\n",
    "Follow the instructions to download and install Ollama on your computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Download LLAVA\n",
    "\n",
    "Now you can use Ollama to download the weights of an LLM from their database. Today you will be using [LLAVA](https://llava-vl.github.io/), which is a multimodal LLM that can perform reasoning on text and images.\n",
    "\n",
    "To download LLAVA open your respective CLI software and run the command:\n",
    "\n",
    "```\n",
    "conda activate aim\n",
    "```\n",
    "```\n",
    "ollama pull llava:7b\n",
    "```\n",
    "\n",
    "After successfully downloading the weights for LLAVA now you can test to see if it works by running:\n",
    "\n",
    "```\n",
    "ollama run llava:7b\n",
    "```\n",
    "\n",
    "Try typing a question into the terminal session to ask LLAVA a question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Run LLAVA via python\n",
    "\n",
    "Now you can try running LLAVA via the python API. Run the following code to test this with text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "\n",
    "from ollama import ChatResponse\n",
    "\n",
    "response: ChatResponse = chat(model='llava:7b', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Why is the sky blue?',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])\n",
    "# or access fields directly from the response object\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Reason about an image with LLAVA\n",
    "\n",
    "Now you will see how you can load in an image and reason about it using LLAVA.\n",
    "\n",
    "First you will need to load in an image. Let's use one from earlier in the term:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "image_path = 'media/simple-mlp.png'\n",
    "im = Image.open('media/simple-mlp.png')\n",
    "im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using the following code you will see how we can give this image to a multimodal LLM and ask it questions relating to the image.\n",
    "\n",
    "This code uses a data stream to get the data as it is being generated. After the response is streamed via ollama it will be printed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.image_helper import get_image_bytes\n",
    "from util.llm_helper import analyze_image_file, stream_parser\n",
    "\n",
    "stream = analyze_image_file(image_path, model='llava:7b', user_prompt='What is in this image?')\n",
    "       \n",
    "parsed = stream_parser(stream)\n",
    "\n",
    "out_text = ''\n",
    "for chunk in parsed:\n",
    "    out_text+=chunk\n",
    "\n",
    "print(out_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Interact with the model in streamlit \n",
    "\n",
    "Now your task is to implement an interactive application in streamlit [week-8b-llava-streamlit-app.py](week-8b-llava-streamlit-app.py)\n",
    "\n",
    "Most of the code is given for you for building the user interface and loading in an image. Your task is to use write using the example about to analyse the image file that the user has uploaded and display a response:\n",
    "\n",
    "**Task 1:** Call the function `analyse_image_file`, you will need to pass in the uploaded image, specify the correct model that you have used in this notebook, and pass in the variable `chat_input` as the user prompt.\n",
    "\n",
    "**task 2:** Use the streamlit function [st.write_stream](https://docs.streamlit.io/develop/api-reference/write-magic/st.write_stream) to display the streamed text. You will need to call the function `stream_parser` first before passing this to the `write_stream` function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
