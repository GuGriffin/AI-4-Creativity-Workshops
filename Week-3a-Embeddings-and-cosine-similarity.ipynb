{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3a: Cosine similarity and vector embeddings\n",
    "\n",
    "This notebook gives an introduction to how embeddings generated by neural networks can be used represent individual data samples like images and text, using [the famous CLIP model from OpenAI](https://openai.com/index/clip/) and how we can use those embeddings to measure and compare how similar to data points are to each other. Before that though, there will be an introduction to the [cosine similarity](https://www.youtube.com/watch?v=5Ao8Ji-f3i8), a very handy mathematical formula for calculating the similarity of two vectors. This is widely used in AI in many domains and tasks, and is of fundamental importance to how powerful modern AI systems like LLMs operate (more on this later in the term).\n",
    "\n",
    "First of all you will need to set up this notebook to use the right environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up your Python environment\n",
    "\n",
    "Before you work through this notebook, please follow the instructions in [Setup-and-test-conda-environment.ipynb](Setup-and-test-conda-environment.ipynb)\n",
    "\n",
    "Once you have done that you will need to make sure that the environment selected to run this notebook and all the other notebooks used in this unit is called `aim`. \n",
    "\n",
    "To do this click the **Select kernel** button in the top right corner of this notebook, and then select `aim`.\n",
    "\n",
    "To make sure that is configured properly, Hit the run cell button (â–¶) on the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aim\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ['CONDA_DEFAULT_ENV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it output the text `aim`?\n",
    "\n",
    "If it does not output the text `aim`, please revisit and follow the instructions in [Setup-and-test-conda-environment.ipynb](Setup-and-test-conda-environment.ipynb).\n",
    "\n",
    "If you still cannot get it working, please raise this with the course instructor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import open_clip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to the Cosine Similarity\n",
    "\n",
    "Cosine similarity is a mathematical formula that we can use to measure how closely related two vectors are to each other. This tells us how similar the direction from the origin is for a set of two vectors by measuring the angle between them. \n",
    "\n",
    "When we calculate the cosine difference we get a number between 1 & -1. The resulting number tells us:\n",
    "- $1$: When the vectors are identical in direction (most similar).\n",
    "- $0$: When the vectors are orthogonal (unrelated).\n",
    "- $-1$: When the vectors are opposite in direction (total opposite).\n",
    "\n",
    "The diagram below gives a visual representation of this:\n",
    "\n",
    "<img src=\"media/cosine_similarity_diagram.png\" alt=\"cosine_similarity_diagram\" width=\"500\"/>\n",
    "\n",
    "The formula for calculating the dot product between two vectors $\\vec{a}$ & $\\vec{b}$ is given as:\n",
    "$$\n",
    "\\text{Cosine Similarity} = \\frac{\\vec{a} \\cdot \\vec{b}}{\\|\\vec{a}\\| \\|\\vec{b}\\|}\n",
    "$$\n",
    "Where $\\vec{a} \\cdot \\vec{b}$ is the [dot product of the two vectors](https://www.youtube.com/watch?v=0iNrGpwZwog), and $\\|\\vec{a}\\| \\|\\vec{b}\\|$ is the product of the [magnitude (aka length from the origin) of the vectors](https://www.youtube.com/watch?v=mGcZGiUn39k) $\\vec{a}$ & $\\vec{b}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine distance in numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_similarity(vec_a, vec_b):\n",
    "        dot_product = vec_a @ vec_b\n",
    "        product_of_magnitudes = np.linalg.norm(vec_a) * np.linalg.norm(vec_b)\n",
    "        return dot_product / product_of_magnitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_1 = np.array([1,0])\n",
    "vec_2 = np.array([0,1])\n",
    "vec_3 = np.array([-1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we get calculate the cosine similarity of the same vector we will get 1 as they are pointing in the same direction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity of vec_1 with vec_1 is: 1.0\n"
     ]
    }
   ],
   "source": [
    "similarity = get_cosine_similarity(vec_1, vec_1)\n",
    "print(f'cosine similarity of vec_1 with vec_1 is: {similarity}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if the magnitude (length) of the vectors is different, if the are pointing in the same direction the cosine distance will be the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vec_1: [1 0]\n",
      "vec_1_scaled: [10  0]\n",
      "cosine similarity of vec_1 with vec_1_scaled is: 1.0\n"
     ]
    }
   ],
   "source": [
    "vec_1_scaled = vec_1 * 10\n",
    "print(f'vec_1: {vec_1}') \n",
    "print(f'vec_1_scaled: {vec_1_scaled}')\n",
    "print(f'cosine similarity of vec_1 with vec_1_scaled is: {similarity}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets compare [orthogonal vectors](https://www.youtube.com/watch?v=6nqMegdbxik):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity of vec_1 with vec_2 is: 0.0\n"
     ]
    }
   ],
   "source": [
    "similarity = get_cosine_similarity(vec_1, vec_2)\n",
    "print(f'cosine similarity of vec_1 with vec_2 is: {similarity}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally lets compare vectors pointing in opposite directions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity of vec_1 with vec_3 is: -1.0\n"
     ]
    }
   ],
   "source": [
    "similarity = get_cosine_similarity(vec_1, vec_3)\n",
    "print(f'cosine similarity of vec_1 with vec_3 is: {similarity}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try experiment with some different vectors (e.g. $\\begin{bmatrix}2 \\\\ 4\\end{bmatrix}$ or $\\begin{bmatrix}-3 \\\\ 5\\end{bmatrix}$) and see what the similarities between them are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity of vec_4 with vec_5 is: -1.0\n"
     ]
    }
   ],
   "source": [
    "vec_4 = np.array([2,4])\n",
    "vec_5 = np.array([-3,5])\n",
    "print(f'cosine similarity of vec_4 with vec_5 is: {similarity}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting embeddings with CLIP\n",
    "\n",
    "The [CLIP model](https://openai.com/index/clip/) from OpenAI to get vector embeddings of both text and images. The CLIP has two component neural networks that both output vectors:\n",
    "- **The text encoder** processes text as input and outputs a vector embedding. \n",
    "- **The image encoder** processes text as input and outputs a vector embedding. \n",
    "\n",
    "What is clever about CLIP is that these vector outputs are in the same 'embedding space', because of this you can compare the vector embeddings of text and images, this is designed such that when you calculate the cosine distance on these vectors it captures **semantically meaningful** relationships between them, even when they represent completely different types of data:\n",
    "\n",
    "<img src=\"media/clip_diagram.png\" alt=\"clip diagram\" width=\"700\"/>\n",
    "\n",
    "The original CLIP was trained on approximately 400 million text and image pairings, that OpenAI scraped from the whole of the world wide web (without permission). The huge amount of data that went into training this model makes these vector embedding representations very powerful. This is a very large and complex neural network compared to what you have been looking at up to this point. For now, you do not need to worry about the details of how this network is built (the classes on computer vision and transformers later in the term will cover that in more detail)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load CLIP\n",
    "\n",
    "This code will load the pre-trained CLIP model from internet onto your PC. Once loaded the code will then show you how many individual weight parameters the CLIP model has:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 151,277,313\n"
     ]
    }
   ],
   "source": [
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='openai')\n",
    "\n",
    "model.eval()\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing an image with CLIP\n",
    "\n",
    "Lets use CLIP to get an embedding for this cute doggo:\n",
    "\n",
    "<img src=\"media/golden-retriever.jpg\" alt=\"golden retriever\" width=\"500\"/>\n",
    "\n",
    "Lets first load in our image from a file using the [PIL (Python image model) library](https://pillow.readthedocs.io/en/stable/reference/Image.html). This will creat a PIL.Image object as a variable to store the image data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'PIL.Image.Image'>\n"
     ]
    }
   ],
   "source": [
    "dog_image = Image.open('media/golden-retriever.jpg').convert(\"RGB\")\n",
    "print(type(dog_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Converting an image to a torch tensor\n",
    "\n",
    "We now need to convert this into a torch tensor. OpenCLIP has a handy `preprocess` function to get the image into the right shape and size for the model. Lets use this to convert the image into a torch tensor. Lets use the `.shape` member variable of this torch tensor to see the dimensionality of this tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "image_tensor = preprocess(dog_image)\n",
    "print(image_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result should be a 3-dimensional tensor. The first dimension are the colour channels (RGB), the second and third dimensions represent the width and height of the image (in pixels). If you put this image directly into the neural network you will get this nasty looking error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 768 but got size 7 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     image_embedding = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_tensor\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/aim/lib/python3.13/site-packages/open_clip/model.py:327\u001b[39m, in \u001b[36mCLIP.encode_image\u001b[39m\u001b[34m(self, image, normalize)\u001b[39m\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mencode_image\u001b[39m(\u001b[38;5;28mself\u001b[39m, image, normalize: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvisual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.normalize(features, dim=-\u001b[32m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m normalize \u001b[38;5;28;01melse\u001b[39;00m features\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/aim/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/aim/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/aim/lib/python3.13/site-packages/open_clip/transformer.py:908\u001b[39m, in \u001b[36mVisionTransformer.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    907\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor):\n\u001b[32m--> \u001b[39m\u001b[32m908\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embeds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    909\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.transformer(x)\n\u001b[32m    910\u001b[39m     pooled, tokens = \u001b[38;5;28mself\u001b[39m._pool(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/aim/lib/python3.13/site-packages/open_clip/transformer.py:789\u001b[39m, in \u001b[36mVisionTransformer._embeds\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    786\u001b[39m x = x.permute(\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# shape = [*, grid ** 2, width]\u001b[39;00m\n\u001b[32m    788\u001b[39m \u001b[38;5;66;03m# class embeddings and positional embeddings\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m x = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m_expand_token\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclass_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[38;5;66;03m# shape = [*, grid ** 2 + 1, width]\u001b[39;00m\n\u001b[32m    791\u001b[39m x = x + \u001b[38;5;28mself\u001b[39m.positional_embedding.to(x.dtype)\n",
      "\u001b[31mRuntimeError\u001b[39m: Sizes of tensors must match except in dimension 1. Expected size 768 but got size 7 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    image_embedding = model.encode_image(image_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding the batch dimension to a torch tensor\n",
    "\n",
    "This is because torch expects the first dimension of the tensor to represent the **batch** dimension for batch processing and training of data. To get the tensor in the right format for CLIP we need to add a **fourth dimension** to the tensor, which by convention should be the first dimension (0). \n",
    "\n",
    "This does not change the data contained in the tensor in any way, it simply adds an extra 'empty' dimension to the tensor (with length 1), though this sounds like a pointless step, it is an essential step when processing data for and extracting data out of pytorch models. \n",
    "\n",
    "To do this you can use the `.unsqueeze()` function. Becuase we want to make the first dimension of the tensor the 'empty' batch dimension, we pass in the index of the first dimension (0) into this function. Run this code and see how the dimension of the tensor has changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "image_tensor = image_tensor.unsqueeze(0)\n",
    "print(image_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets try processing that image again with CLIP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    image_embedding = model.encode_image(image_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can see the shape of this tensor. It should be a tensor with the shape 1x512. Though technically a matrix, this is essentially a vector with the first dimension (used for batch processing) being 'empty' with a length of 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "print(image_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get rid of this empty dimension and convert this into a proper vector we can use the function `.squeeze()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "image_embedding = image_embedding.squeeze()\n",
    "print(image_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing text with CLIP\n",
    "\n",
    "Now lets process some text with CLIP. Lets take the following sentence as a string:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Happy golden retriever with its mouth open and tongue out, standing on green grass\n"
     ]
    }
   ],
   "source": [
    "dog_text = '\"Happy golden retriever with its mouth open and tongue out, standing on green grass'\n",
    "print(dog_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets tokenize this text using the openCLIP text tokenizer. This will give us a vector of length 77 tokens (with the empty batch dimension created for you this time by the tokenizer), which is the maximum number of tokens in a text string CLIP can process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 77])\n"
     ]
    }
   ],
   "source": [
    "text_tokens_tensor = open_clip.tokenizer.tokenize(dog_text)\n",
    "print(text_tokens_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is essentially just a list of integers, each one being the number that represents that text token fragment. [This website has a nice interactive demo of the OpenAI tokenizers used for the GPT models](https://platform.openai.com/tokenizer). You can actually see the list of tokens if you print the data contained in this tensor directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[49406,   257,   900,  3878, 28394,   593,   902,  4932,  1488,   537,\n",
      "         13626,   620,   267,  2862,   525,  1901,  5922, 49407,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0]])\n"
     ]
    }
   ],
   "source": [
    "print(text_tokens_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can encode this list of tokens with CLIP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    text_embedding = model.encode_text(text_tokens_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which just like when encoding images, gives a tensor with the shape 1x512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "print(text_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which can be converted into a proper vector using `squeeze()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "text_embedding = text_embedding.squeeze()\n",
    "print(text_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating simiarity with CLIP embeddings \n",
    "\n",
    "Now you have our two vector embeddings (`image_embedding` and `text_embedding`). Now you need to convert these vector embeddings from torch tensors to numpy arrays using the `.numpy()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "image_embedding = image_embedding.numpy()\n",
    "text_embedding = text_embedding.numpy()\n",
    "print(type(image_embedding))\n",
    "print(type(text_embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now compare the similarity of these vector embeddings using the function `get_cosine_similarity` from the [cosine similarity section at the start of the notebook](#introduction-to-the-cosine-similarity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3275515\n"
     ]
    }
   ],
   "source": [
    "similarity = get_cosine_similarity(image_embedding, text_embedding)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should give a similarity score of approximately $0.26$. Can you edit the text description given in the string variable `dog_text` and re-run the this code to get a closer match?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "Now you have the basics down, you can now move onto the next notebook [Week-3b-Downloading-and-processing-museum-dataset-with-CLIP.ipynb](Week-3b-Downloading-and-processing-museum-dataset-with-CLIP.ipynb) where you will be using CLIP to download a dataset of images from a museum\n",
    "collection, and using CLIP to create vector embeddings for each image.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
