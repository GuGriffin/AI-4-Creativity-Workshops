{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5: Speech transcription with whisper\n",
    "\n",
    "This notebook gives an introduction to the [whisper model](https://openai.com/index/whisper/) by OpenAI that can transcribe raw audio and return text. In this notebook you will see a bit about how whisper works and there will be some tasks for to preprocess recorded audio for making it suitable to be processed by whisper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up your Python environment\n",
    "\n",
    "Before you work through this notebook, please follow the instructions in [Setup-and-test-conda-environment.ipynb](Setup-and-test-conda-environment.ipynb)\n",
    "\n",
    "Once you have done that you will need to make sure that the environment selected to run this notebook and all the other notebooks used in this unit is called `aim`. \n",
    "\n",
    "To do this click the **Select kernel** button in the top right corner of this notebook, and then select `aim`.\n",
    "\n",
    "To make sure that is configured properly, Hit the run cell button (â–¶) on the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.environ['CONDA_DEFAULT_ENV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it output the text `aim`?\n",
    "\n",
    "If it does not output the text `aim`, please revisit and follow the instructions in [Setup-and-test-conda-environment.ipynb](Setup-and-test-conda-environment.ipynb).\n",
    "\n",
    "If you still cannot get it working, please raise this with the course instructor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install python packages\n",
    "\n",
    "Here you now need to install the openai-whisper and pyttsx3 (python speech to text 3) libraries to be used in this and subsequent notebooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai-whisper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries\n",
    "\n",
    "Now load the libraries you will need for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import whisper\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "from IPython.display import Audio\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and transcribe audio\n",
    "\n",
    "Listen to this [reenactment by Thomas Edison](https://publicdomainreview.org/collection/edison-reading-mary-had-a-little-lamb-1927/), which was recorded on the 50th anniversary of [his patent for the 'phonograph'](https://www.wnyc.org/story/86604-today-in-history-thomas-edison/) the first ever device for recording audio. The was a reenactment of the first ever recording of audio in the phonograph, and in it Thomas Edison read out loud the nursery rhyme [Mary had a little lamb](https://en.wikipedia.org/wiki/Mary_Had_a_Little_Lamb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(\"media/audio/thomas-edison-mary-had-a-little-lamb.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use whisper to transcribe it into text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = whisper.load_model(\"turbo\")\n",
    "audio = whisper.load_audio(\"media/audio/thomas-edison-mary-had-a-little-lamb.mp3\")\n",
    "result = model.transcribe(audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now you can see the results. How well did it perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whisper transcription step-by-step\n",
    "\n",
    "The function `model.trascribe()` has hidden a number of things from what is really going on. Now lets take a deeper look to see how it is processsing the data.\n",
    "\n",
    "First lets take a look at the what is in the variable `audio`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(audio))\n",
    "print(audio.shape)\n",
    "print(audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to visualise the waveform with matplotlib. The x-axis represents the samples and the y-axis represents the amplitude of the audio signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the waveform\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(audio)\n",
    "plt.title(\"Audio Waveform\")\n",
    "plt.xlabel(\"Sample index\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whisper does not process the waveform directly, occasionally this is done with neural networks, but it can be very inefficient.In machine learning and many other areas of audio signal processing, [spectrograms](https://en.wikipedia.org/wiki/Spectrogram) are an alternative representation of audio (as a collection of frequencies and their relative strength) that is a much more efficient (compressed) representation of an audio signal.\n",
    "\n",
    "Run the next cell to take a look at the spectrogram of this audio signal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = librosa.stft(audio)  \n",
    "S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
    "plt.figure()\n",
    "librosa.display.specshow(S_db)\n",
    "plt.title(\"Spectrogram\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.colorbar(format=\"%+2.0f dB\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This however is still the not the most efficient representation available for the task of speech processing. The [Mel scale](https://en.wikipedia.org/wiki/Mel_scale) is a an alternative scale for representing frequencies that aligns much more closely with the human perception of different pitches. For tasks like speech processing this is particularly useful because speech and (how the interpret it) is very closely tied up with how humans understand and perceive audio signals. \n",
    "\n",
    "Run the next cell the convert the audio signal into a mel spectrogram (a frequency spectrogram in the mel scale) to see what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel = whisper.log_mel_spectrogram(audio, n_mels=model.dims.n_mels)\n",
    "\n",
    "# Plot the Mel spectrogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(mel.numpy(), aspect='auto', origin='lower', cmap='inferno', interpolation='none')\n",
    "plt.title(\"Mel Spectrogram\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Frequency (Mel scale)\")\n",
    "plt.colorbar(format=\"%+2.0f dB\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the next cell to try and decode this mel-spectrogram with the whisper network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = whisper.DecodingOptions()\n",
    "result = whisper.decode(model, mel, options)\n",
    "\n",
    "print(result.language)\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will have got an error. \n",
    "\n",
    "This is because whisper is built to process audio in exactly 30-second blocks. To get decode the mel-spectrogram you first have to the util function in whisper called `pad_or_trim`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = whisper.pad_or_trim(audio, 16000*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now look at what this signal looks like now as a spectrogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the waveform\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(audio)\n",
    "plt.title(\"Audio Waveform\")\n",
    "plt.xlabel(\"Sample index\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as a mel-spectrogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel = whisper.log_mel_spectrogram(audio, n_mels=model.dims.n_mels)\n",
    "mel_np = mel.numpy()\n",
    "\n",
    "# Plot the Mel spectrogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(mel_np, aspect='auto', origin='lower', cmap='inferno', interpolation='none')\n",
    "plt.title(\"Mel Spectrogram\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Frequency (Mel scale)\")\n",
    "plt.colorbar(format=\"%+2.0f dB\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should be able to finally decode this without an error. If the execution of the next cell takes over 10 min, feel free to skip this and jump to **Task 1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = whisper.DecodingOptions()\n",
    "result = whisper.decode(model, mel, options)\n",
    "\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whisper can an also make predictions about the language that is being spoken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, probs = model.detect_language(mel)\n",
    "print(f\"Detected language: {max(probs, key=probs.get)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Transcribing audio recorded from your laptop microphone\n",
    "\n",
    "Now you are going to use whisper to transcribe audio from your mic.\n",
    "\n",
    "The following function will record audio from your microphone input on your laptop using the [Python sounddevice](https://python-sounddevice.readthedocs.io/en/0.5.1/) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_audio(duration=5, samplerate=44100):\n",
    "    audio = sd.rec(int(duration * samplerate), samplerate=samplerate, channels=1, dtype=np.int16)\n",
    "    print('recording')\n",
    "    sd.wait()\n",
    "    print('recording complete')\n",
    "    return audio, samplerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to record 5 seconds from your microphone. Say something that you want to be transcribed by whisper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_recording, samplerate = record_audio()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should give you a numpy array of shape (220500, 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(audio_recording))\n",
    "print(audio_recording.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can listen to this back, though the `Audio` player expects a vector not a matrix, hence you need to call `.squeeze()` to remove the 'empty' dimensions of the numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(data=audio_recording.squeeze(), rate=44100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now take a look at this spectrogram. What do you notice that is different from the spectrograms from before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the waveform\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(audio_recording.squeeze())\n",
    "plt.title(\"Audio Waveform\")\n",
    "plt.xlabel(\"Sample index\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few things different about this waveform from the other one. The next steps will walk you through this one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Convert matrix to vector\n",
    "\n",
    "Use the either the `.squeeze()` or `.flatten()` function to convert the numpy array `audio_recording` from a matrix to a vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_recording ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert audio_recording.shape == (220500,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: Normalise audio\n",
    "\n",
    "First you will need to normalise the audio to be between -1 & 1. You will need to change the data type of the numpy array from int16 to float32. Use the `np.astype()` function to do this.\n",
    "\n",
    "After you have done this divide every value in the numpy array by 32768.0 to normalise the amplitudes to be range -1 & 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_recording = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert audio_recording.dtype == np.float32\n",
    "assert audio_recording.max() < 1\n",
    "assert audio_recording.min() > -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And see what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the waveform\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(audio_recording)\n",
    "plt.title(\"Audio Waveform\")\n",
    "plt.xlabel(\"Sample index\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And see what it looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: Resample audio\n",
    "\n",
    "Whisper has been trained on audio that is at a sample rate of 16000, which is quite a bit lower than the sample rate of 441000 that is used as standard in audio recording and playback. This lower sample rate is used because human speech does mostly happens in a lower frequencies that humans can perceive. \n",
    "\n",
    "Use the [resample function from librosa](https://librosa.org/doc/main/generated/librosa.resample.html) to resample the audio from 44100 to 16000:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_recording = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(audio_recording.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert audio_recording.shape == (80000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And see what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the waveform\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(audio_recording)\n",
    "plt.title(\"Audio Waveform\")\n",
    "plt.xlabel(\"Sample index\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4: Zero pad the audio to 30 seconds long\n",
    "\n",
    "Now you need to take the 5 second long audio section and pad it to be 30 seconds long as that is what whisper has been trained on.\n",
    "\n",
    "Use the function `pad_or_trim` from whisper to pad the audio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_recording = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now test the code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert audio_recording.shape == (480000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now visualise it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the waveform\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(audio_recording)\n",
    "plt.title(\"Audio Waveform\")\n",
    "plt.xlabel(\"Sample index\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now test it with whisper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.transcribe(audio_recording)\n",
    "print(result['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 5: Bring it all together\n",
    "\n",
    "Take the steps from tasks 1-4 and put them all together into the function, `preprocess_audio`. Make sure you have default values for `original_sr` and `target_sr`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(audio_input, original_sr = , target_sr = ):\n",
    "    #\n",
    "    # Your code here\n",
    "    # \n",
    "    return # return processed audio here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test this code with a new recording. Make sure to say something as the recording is happening:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_recording_test, samplerate = record_audio()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test your function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_audio = preprocess_audio(audio_recording_test)\n",
    "\n",
    "assert processed_audio.shape == (480000,)\n",
    "assert processed_audio.dtype == np.float32\n",
    "assert processed_audio.max() < 1\n",
    "assert processed_audio.min() > -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now see the transcription with whisper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.transcribe(processed_audio)\n",
    "print(result['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Building a conversational app \n",
    "\n",
    "**Step A:** Now take the functions `record_audio` and `preprocess_audio` and copy and paste them into [week-5b-voice-chatbot-streamlit-app.py](week-5b-voice-chatbot-streamlit-app.py). \n",
    "\n",
    "**Step B:** Replace the `print` calls with `st.write` calls to display the information to the user in the app.\n",
    "\n",
    "**Step C:** In the code block `if st.button(\"Record and Chat\"):` write code that records audio from the microphone, preprocesses it, transcribes it from whisper and puts the resulting text into a variable called `user_text`.\n",
    "\n",
    "**Step D:** In the code block `if user_text:` Pass the variable user text into the function `get_llm_response` to get a response to the users input from [SmolLM](https://huggingface.co/blog/smollm) (this code is provided for you). After getting the response display it to web interface using `st.write`. \n",
    "\n",
    "**Step E:** Run `streamlit run week-5b-voice-chatbot-streamlit-app.py` to test out your voice activated chatbot.\n",
    "\n",
    "\n",
    "## Taking it further\n",
    "\n",
    "To take this further you can try either of the following:\n",
    "\n",
    "- Use a more advanced instruct-LLM transformer model from the [huggingface transformers library](https://huggingface.co/docs/transformers/en/index) to get better responses. \n",
    "- Find and use a text-to-speech model in pytorch to get the code to read back the answers to the user (you may want to create or clone a new conda environment for this)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
