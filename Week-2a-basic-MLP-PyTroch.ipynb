{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2a: Building and training a simple MLP in PyTorch\n",
    "\n",
    "This notebook gives a very simple illustration of how to build the most basic 2-layer MLP example in PyTorch. For simplicity, many of the normal things we do when building and training neural network models in PyTorch have been left out (such as biases, [activation functions](https://pytorch.org/docs/stable/nn.functional.html#non-linear-activation-functions), and [loading and iterating on datasets when training](https://pytorch.org/docs/stable/data.html)). In the second notebook you will get the to build and train a network with all of these components. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up your Python environment\n",
    "\n",
    "Before you work through this notebook, please follow the instructions in [Setup-and-test-conda-environment.ipynb](Setup-and-test-conda-environment.ipynb)\n",
    "\n",
    "Once you have done that you will need to make sure that the environment selected to run this notebook and all the other notebooks used in this unit is called `aim`. \n",
    "\n",
    "To do this click the **Select kernel** button in the top right corner of this notebook, and then select `aim`.\n",
    "\n",
    "To make sure that is configured properly, Hit the run cell button (â–¶) on the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.environ['CONDA_DEFAULT_ENV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it output the text `aim`?\n",
    "\n",
    "If it does not output the text `aim`, please revisit and follow the instructions in [Setup-and-test-conda-environment.ipynb](Setup-and-test-conda-environment.ipynb).\n",
    "\n",
    "If you still cannot get it working, please raise this with the course instructor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a simple MLP \n",
    "\n",
    "Below is the code in PyTorch that builds this simple MLP network:\n",
    "\n",
    "![MLP diagram](../media/simple-mlp.png)\n",
    "\n",
    "In PyTorch you build neural networks by creating a class that [inherits](https://www.w3schools.com/python/python_inheritance.asp) from the [torch.nn.Module class](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). \n",
    "\n",
    "For any network you need to define two functions: \n",
    "- The constructor (`__init__`) where you create the layers of the network.\n",
    "- The `forward` function, where you define how the data `x` is is processed by the network, and the sequence in which the computation of layers is performed.\n",
    "\n",
    "Here we are creating an MLP with 2 fully connected layers (a unique weighted connection between each input and output). To do this we use the [nn.Linear layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html). For each layer you define the number of inputs and outputs, pytorch will automatically create and populate the weight matrix to match these dimensions.\n",
    "\n",
    "In the `forward` function you then have to specify which the chain of computation for the network. Here the data is processed first by the hidden layer (`x = self.hidden(x)`) and then the output layer `x = self.output(x)`), the code for performing the matrix-vector multiplication occurs here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.hidden = nn.Linear(2, 2, bias=False)\n",
    "        self.output = nn.Linear(2, 1, bias=False) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x) \n",
    "        x = self.output(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating the mlp, optimiser and loss function\n",
    "\n",
    "Now we need to instantiate our network, which we will call `mlp`. We also need to decided what [loss function](https://pytorch.org/docs/stable/nn.html#loss-functions) we are using to evaluate our networks outputs, and the [optimiser](https://pytorch.org/docs/stable/optim.html) that will be used to update the weights of the network in training. \n",
    "\n",
    "There are many options to choose from, but for this example we will use [L1 loss](https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html) (absolute error) and [stochastic gradient descent optimiser](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = SimpleMLP()\n",
    "criterion = nn.L1Loss() \n",
    "optimizer = optim.SGD(mlp.parameters(), lr=0.001) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see the weights of the network, just like the task from last week, we have a 2x2 matrix to represent the weights of the first layer and a vector of length 2 for the weights of the second layer. PyTorch will create random values for these weights between -1,1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hidden layer weights:\", mlp.hidden.weight.data)\n",
    "print(\"Output layer weights:\", mlp.output.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define input data and true value of prediction\n",
    "\n",
    "Lets try to recreate the example from task A of last weeks worksheet, where the input to this network was a vector $\\vec{x} = \\begin{bmatrix}2 \\\\ 4\\end{bmatrix}$ and the correct output was $70$. \n",
    "\n",
    "We now need to make these values as [pytorch tensors](https://pytorch.org/docs/stable/tensors.html#torch.Tensor), which are designed to behave a lot like numpy arrays, but are integrated into pytorch's tracking of computational steps in order to backpropagate gradients when training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = torch.tensor([2,4], dtype=torch.float)\n",
    "target_output = torch.tensor([70], dtype=torch.float)\n",
    "print(input_data)\n",
    "print(target_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets process our output and see what the model predicts. By default, if we pass data into our model `mlp(input_data)` this will automatically call the `forward` function that [we defined earlier](#building-a-simple-mlp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = mlp(input_data)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets evaluate this prediction against the target output by using our loss function (`criterion`). This calculates the absolute difference between the prediction $p$ and the target value $t$ \n",
    "\n",
    "$loss = \\lvert p - t \\rvert$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(prediction, target_output)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have completed the forward pass, we now need to perform the backward pass, to calculate the gradients that will be used to update the weight parameters of the network based on the error. To do that we call the function `.backward()` on the tensor variable `loss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight for each layer has a variable `.grad` that stores the gradient for the weight. This has the same dimensionality as the value of the weights. \n",
    "\n",
    "In essence the values here represent how much each weight parameter has contributed to the error, large numbers (positive or negative) mean these weights contributed more to getting an incorrect result, where parameters with low numbers (closer to 0) contributed less to the overall error in prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hidden layer gradients:\", mlp.hidden.weight.grad)\n",
    "print(\"Output layer gradients:\", mlp.output.weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we have computed the gradients, the weights of the network have not yet been changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hidden layer weights:\", mlp.hidden.weight.data)\n",
    "print(\"Output layer weights:\", mlp.output.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To update the weights of the network based on the gradients, we need to call the `.step()` function on our optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the weights will have been adjusted by the optimiser using gradients calculated by pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hidden layer weights:\", mlp.hidden.weight.data)\n",
    "print(\"Output layer weights:\", mlp.output.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we process the data again with the network, we should get a different result, hopefully that is closer to the value we want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = mlp(input_data)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "Now lets run this training loop sequence for 1000 iterations, hopefully the loss will iteratively go down as the predictions from the model get closer to the target value of 70. \n",
    "\n",
    "However, this is a very small model and whether it trains successfully is dependent on the initial conditions (the choice of random weights) of the network. \n",
    "\n",
    "If the model does not improve over the course of training, try hitting the `restart` and `run all` buttons at the top of this notebook to re-run the code with different initial parameters for the weights and see if you can get a model that comes close to giving a correct prediction? You can also try increasing the number of steps (`num_steps`) to train the model for longer if it needs more training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 1000\n",
    "for step in range(num_steps):\n",
    "    optimizer.zero_grad()\n",
    "    mlp.zero_grad()\n",
    "    prediction = mlp(input_data)\n",
    "    loss = criterion(prediction, target_output)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (step + 1) % 100 == 0:\n",
    "        print(f\"Step [{step + 1}/{step}], Loss: {loss.item():.4f}, mlp prediction: {prediction.item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the weights now. Are they close to the values of the weights in last weeks worksheet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hidden layer weights:\", mlp.hidden.weight.data)\n",
    "print(\"Output layer weights:\", mlp.output.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have seen your first example of building and training a neural network in PyTorch. In the next notebook you will look at how training is performed with a dataset containing lots of training examples, you will also get to build your own neural networks and experiment with different numbers of layers, and units in the layers of the networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "- **Task 1:** Are you able to train a network that gets close to making the correct prediction? Try a few times running this notebook to see if you can get one that gives the correct answer. You may need to increase the number of training steps represented by the variable `num_steps` in the [training loop](#training-loop) cell.\n",
    "- **Task 2:** Adapt this code to remake the model from Task 2 of last weeks worksheet shown in the follow diagram and train it to predict the output value 68 based on the vector input $\\vec{x} = \\begin{bmatrix}3 \\\\ 1 \\\\ 4\\end{bmatrix}$:\n",
    "![MLP task B diagram](media/mlp-task-b.png)\n",
    "> **Tip:** You will need to change the number of inputs and/or outputs of the layers in [the cell when the MLP is defined](#building-a-simple-mlp). You will also need to [edit the input data and target value for prediction](#define-input-data-and-true-value-of-prediction).\n",
    "- **Task 3:** Adapt this code to remake the model from Task 2 of last weeks worksheet shown in the follow diagram and train it to predict the output value 39 based on the vector input $\\vec{x} = \\begin{bmatrix}3 \\\\ 1\\end{bmatrix}$:\n",
    "![MLP task C diagram](media/mlp-task-c.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
