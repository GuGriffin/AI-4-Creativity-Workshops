{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UhG5CzHQWNzr"
   },
   "source": [
    "# Week 6a: Image generation with stable diffusion\n",
    "\n",
    "In this notebook, you will be looking at text to image generation with Stable Diffusion and later some different ways to make animations with text to image diffusion techniques. Following that, there are some tasks to build an interface around different parts of functionality in streamlit. \n",
    "\n",
    "Today you will be using [kjsman's simplified (and hackable!) stable diffusion PyTorch implementation](https://github.com/kjsman/stable-diffusion-pytorch)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up your Python environment\n",
    "\n",
    "Before you work through this notebook, please follow the instructions in [Setup-and-test-conda-environment.ipynb](Setup-and-test-conda-environment.ipynb)\n",
    "\n",
    "Once you have done that you will need to make sure that the environment selected to run this notebook and all the other notebooks used in this unit is called `aim`. \n",
    "\n",
    "To do this click the **Select kernel** button in the top right corner of this notebook, and then select `aim`.\n",
    "\n",
    "To make sure that is configured properly, Hit the run cell button (â–¶) on the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aim\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ['CONDA_DEFAULT_ENV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it output the text `aim`?\n",
    "\n",
    "If it does not output the text `aim`, please revisit and follow the instructions in [Setup-and-test-conda-environment.ipynb](Setup-and-test-conda-environment.ipynb).\n",
    "\n",
    "If you still cannot get it working, please raise this with the course instructor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download model weights\n",
    "\n",
    "Next, try downloading the weights for stable diffusion with curl, if that doesn't work, just copy and paste the url directly into your browser and it will start downloading. You will need to move the file into the same folder as this notebook to run the next command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NXnKKOxcMsin"
   },
   "outputs": [],
   "source": [
    "!curl -OL https://huggingface.co/jinseokim/stable-diffusion-pytorch-data/resolve/main/data.v20221029.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then unpack the following file using the following instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xf data.v20221029.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "fGOopQsDS-7U"
   },
   "outputs": [],
   "source": [
    "#@title Preload models (takes about ~20 seconds on default settings)\n",
    "import os\n",
    "import numpy as np\n",
    "import IPython.display\n",
    "\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "from stable_diffusion_pytorch import pipeline\n",
    "from stable_diffusion_pytorch import model_loader\n",
    "\n",
    "%matplotlib inline\n",
    "models = model_loader.preload_models('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UyM8vbLnWVNP"
   },
   "source": [
    "## Generate images from text\n",
    "\n",
    "The first task is to familiarise yourself with image generation using stable diffusion. \n",
    "\n",
    "To begin with we will be generating images from random noise using a text string. Experiment with different prompts and different generation parameters to familarise yourself with all of the possible ways of configuring this kind of model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "x_dhQfFYXoPu"
   },
   "outputs": [],
   "source": [
    "#Positive prompt\n",
    "prompt = \"an astronaut riding a horse\"  #@param { type: \"string\" }\n",
    "prompts = [prompt]\n",
    "\n",
    "#Negative prompt\n",
    "uncond_prompt = \"\" #@param { type: \"string\" }\n",
    "uncond_prompts = [uncond_prompt] if uncond_prompt else None\n",
    "\n",
    "#Generation parameters\n",
    "device = 'cpu' #@param {\"cpu\", \"cuda\", \"mps\"]\n",
    "strength = 0.8  #@param { type:\"slider\", min: 0, max: 1, step: 0.01 }\n",
    "do_cfg = True  #@param { type: \"boolean\" }\n",
    "cfg_scale = 7.5  #@param { type:\"slider\", min: 1, max: 14, step: 0.5 }\n",
    "height = 512  #@param { type: \"integer\" }\n",
    "width = 512  #@param { type: \"integer\" }\n",
    "sampler = \"k_lms\"  #@param [\"k_lms\", \"k_euler\", \"k_euler_ancestral\"]\n",
    "n_inference_steps = 20  #@param { type: \"integer\" }\n",
    "\n",
    "use_seed = False  #@param { type: \"boolean\" }\n",
    "if use_seed:\n",
    "    seed = 42  #@param { type: \"integer\" }\n",
    "else:\n",
    "    seed = None\n",
    "\n",
    "image = pipeline.generate(prompts=prompts, uncond_prompts=uncond_prompts,\n",
    "                  input_images=[], strength=strength,\n",
    "                  do_cfg=do_cfg, cfg_scale=cfg_scale,\n",
    "                  height=height, width=width, sampler=sampler,\n",
    "                  n_inference_steps=n_inference_steps, seed=seed,\n",
    "                  models=models, device=device, idle_device='cpu')[0]\n",
    "\n",
    "imshow(np.asarray(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make animations with stable diffusion\n",
    "\n",
    "\n",
    "The following code block will produce a 'zooming' animation, by feeding the outputs of the diffusion model back in as inputs. First run it and inspect the code to get a feel for how it works. \n",
    "\n",
    "Once you have ran the code once, can you modify the previous code block to do one of more of the following:\n",
    "\n",
    "\n",
    "\n",
    "### Using FFMPEG\n",
    "\n",
    "If you want to make the gif animation using this code notebook you will need to install [ffmpeg](https://ffmpeg.org/download.html). We have already installed ffmpeg in week 4, so you should already have it. But if you don't, then you can follow these instructions:\n",
    "\n",
    "To install FFMPEG on Mac:\n",
    "- Step 1: [install homebrew](https://brew.sh/)\n",
    "- Step 2: Run `brew install ffmpeg`\n",
    "\n",
    "To install FFMPEG on Windows:\n",
    "- Follow [these instructions](https://phoenixnap.com/kb/ffmpeg-windows) for Windows installation\n",
    "\n",
    "To install FFMPEG on Ubuntu linux:\n",
    "- Step 1: Run `sudo apt update`\n",
    "- Step 2: Run `sudo apt install ffmpeg`\n",
    "\n",
    "If you cannot install FFMPEG you can [make a gif manually using this website](https://ezgif.com/maker).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Positive prompt\n",
    "prompt = \"Galaxies in outer space, photograph from the Hubble telescope\"  #@param { type: \"string\" }\n",
    "prompts = [prompt]\n",
    "\n",
    "#Negative prompt\n",
    "uncond_prompt = \"\" #@param { type: \"string\" }\n",
    "uncond_prompts = [uncond_prompt] if uncond_prompt else None\n",
    "\n",
    "#Parameters for generation\n",
    "device = 'cpu' #@param {\"cpu\", \"cuda\", \"mps\"]\n",
    "strength = 0.8  #@param { type:\"slider\", min: 0, max: 1, step: 0.01 }\n",
    "do_cfg = True  #@param { type: \"boolean\" }\n",
    "cfg_scale = 7.5  #@param { type:\"slider\", min: 1, max: 14, step: 0.5 }\n",
    "height = 512  #@param { type: \"integer\" }\n",
    "width = 512  #@param { type: \"integer\" }\n",
    "sampler = \"k_lms\"  #@param [\"k_lms\", \"k_euler\", \"k_euler_ancestral\"]\n",
    "n_inference_steps = 15 \n",
    "\n",
    "crop_dim = 25 # How many pixels to crop by on each side of the image\n",
    "\n",
    "use_seed = False  #@param { type: \"boolean\" }\n",
    "if use_seed:\n",
    "    seed = 42  #@param { type: \"integer\" }\n",
    "else:\n",
    "    seed = None\n",
    "\n",
    "output_folder = 'animation_frames'\n",
    "if not os.path.isdir(output_folder):\n",
    "    os.mkdir(output_folder)\n",
    "\n",
    "input_image_array = []\n",
    "#Create a high quality initial image for the animation\n",
    "init_image = pipeline.generate(prompts=prompts, uncond_prompts=uncond_prompts,\n",
    "              input_images=input_image_array, strength=strength,\n",
    "              do_cfg=do_cfg, cfg_scale=cfg_scale,\n",
    "              height=height, width=width, sampler=sampler,\n",
    "              n_inference_steps=n_inference_steps, seed=seed,\n",
    "              models=models, device=device, idle_device='cpu')[0]\n",
    "\n",
    "init_im_crop = init_image.crop((crop_dim, crop_dim, init_image.size[0] - crop_dim, init_image.size[1] - crop_dim))\n",
    "# Unless you want to get experimental we probably just want to have one image in this array at any one time\n",
    "input_image_array = [init_im_crop]\n",
    "\n",
    "#Reduce the strength and number of iterations for generation the frames in the animation\n",
    "strength = 0.3\n",
    "cfg_scale = 3\n",
    "n_inference_steps = 5 \n",
    "\n",
    "# This block of code will recursively crop and feed in the previously generated image into the model\n",
    "for i in range(10):\n",
    "    image = pipeline.generate(prompts=prompts, uncond_prompts=uncond_prompts,\n",
    "                      input_images=input_image_array, strength=strength,\n",
    "                      do_cfg=do_cfg, cfg_scale=cfg_scale,\n",
    "                      height=height, width=width, sampler=sampler,\n",
    "                      n_inference_steps=n_inference_steps, seed=seed,\n",
    "                      models=models, device='cpu', idle_device='cpu')[0]\n",
    "    \n",
    "    num_str = str(i+1).zfill(2)\n",
    "    # Save generated image\n",
    "    image.save(f'{output_folder}/frame_{num_str}.jpg')\n",
    "    # Crop the output image\n",
    "    im_crop = image.crop((crop_dim, crop_dim, image.size[0] - crop_dim, image.size[1] - crop_dim))\n",
    "    # Replace image in input image array with new_image\n",
    "    input_image_array = [im_crop]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Animate frames into a short gif using ffmpeg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ffmpeg -y -f image2 -framerate 4 -i 'animation_frames/frame_%02d.jpg' -loop 0 ai_animation.gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Image('ai_animation.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reimagine an animation using stable diffusion\n",
    "    \n",
    "The following code block has code to modify the first frame of an animation based on a text prompt, can you modify the code to modify all 10 frames in the animation and create a new reimaging of Eadweard Muybridge's 'The horse in motion' based on your text to image prompt?\n",
    "\n",
    "Once you have done that can you do the same thing with a different animation you have found? \n",
    "\n",
    "You can extract frames from a gif using: \n",
    "`ffmpeg -i input/my_animation.gif -vsync 0 frames/frame_%02d.png`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Positive prompt\n",
    "prompt = \"an astronaut riding a horse\"  #@param { type: \"string\" }\n",
    "prompts = [prompt]\n",
    "\n",
    "#Negative prompt\n",
    "uncond_prompt = \"\" #@param { type: \"string\" }\n",
    "uncond_prompts = [uncond_prompt] if uncond_prompt else None\n",
    "\n",
    "device = 'cpu' #@param {\"cpu\", \"cuda\", \"mps\"]\n",
    "strength = 0.8  #@param { type:\"slider\", min: 0, max: 1, step: 0.01 }\n",
    "do_cfg = True  #@param { type: \"boolean\" }\n",
    "cfg_scale = 7.5  #@param { type:\"slider\", min: 1, max: 14, step: 0.5 }\n",
    "height = 512  #@param { type: \"integer\" }\n",
    "width = 512  #@param { type: \"integer\" }\n",
    "sampler = \"k_lms\"  #@param [\"k_lms\", \"k_euler\", \"k_euler_ancestral\"]\n",
    "n_inference_steps = 10  #@param { type: \"integer\" }\n",
    "\n",
    "use_seed = False  #@param { type: \"boolean\" }\n",
    "if use_seed:\n",
    "    seed = 42  #@param { type: \"integer\" }\n",
    "else:\n",
    "    seed = None\n",
    "\n",
    "input_folder = 'media/horse-in-motion'\n",
    "output_folder = 'output_animation'\n",
    "\n",
    "if not os.path.isdir(output_folder):\n",
    "    os.mkdir(output_folder)\n",
    "    \n",
    "for i in range(1,12):\n",
    "    num_str = str(i).zfill(2)\n",
    "    input_image_array = [Image.open(f'{input_folder}/frame_{num_str}.jpg')]\n",
    "    image = pipeline.generate(prompts=prompts, uncond_prompts=uncond_prompts,\n",
    "                    input_images=input_image_array, strength=strength,\n",
    "                    do_cfg=do_cfg, cfg_scale=cfg_scale,\n",
    "                    height=height, width=width, sampler=sampler,\n",
    "                    n_inference_steps=n_inference_steps, seed=seed,\n",
    "                    models=models, device=device, idle_device='cpu')[0]\n",
    "    image.save(f'{output_folder}/frame_{num_str}.jpg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can animate the frames into a short gif using ffmpeg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ffmpeg -y -f image2 -framerate 4 -i 'output_animation/frame_%02d.jpg' -loop 0 horse_ai_animation.gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Image('horse_ai_animation.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook's tasks\n",
    "\n",
    "**Task 1:** Experiment with the settings in the [image generation cell](#generate-images-from-text). Try using different numbers of iterations for sampling, a different sampler and different config strengths to see how the effects the image generation process.\n",
    "\n",
    "**Task 2:** Now move onto the [zooming animation cell](#make-animations-with-stable-diffusion), can you adjust the code to do any of the following:\n",
    "- Make a different animation with a different prompt\n",
    "- Make a longer animation \n",
    "- Improve the fidelity of the animation by adjusting the generation parameters\n",
    "- Make an animation that zooms slower or more quickly\n",
    "- Use an array of text prompts to make an animation that changes theme over time\n",
    "\n",
    "**Task 3** Now move onto the [reimagining animation cell](#reimagine-an-animation-using-stable-diffusion). Can you load in a different gif or animation and use a diffusion model to edit that.\n",
    "\n",
    "### Tasks (for streamlit app)\n",
    "\n",
    "**Task 1:** First run the app [week-6b-diffusion-streamlit-app.py](week-6b-diffusion-streamlit-app.py) to make sure you can get it running.\n",
    "\n",
    "**Task 2:** Add some extra features to this app such as:\n",
    "- A user loading in an image and having that as the starting point for generating an image.\n",
    "- Borrowing code from this notebook for a user to make an animation in the app.\n",
    "- Dividing the prompt up into parts for a structured interface based on questions like CCI's own [imagination tool](https://www.youtube.com/live/N5qledsSzCU?si=LAqFDNfG54qfkfmH&t=15576).\n",
    "- Adding more of the hyper-parameters as configurable steps in the interface."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "iDI2dKfRWTId"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "aim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
